name: Assiny Scraper Automation

on:
  schedule:
    - cron: "*/30 * * * *"   # a cada 30 min (UTC)
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    env:
      STORAGE_STATE_JSON: ${{ secrets.STORAGE_STATE_JSON }}
      GIT_USER_NAME: "github-actions"
      GIT_USER_EMAIL: "github-actions@github.com"

    steps:
      # 1️⃣ Baixar o código
      - name: Checkout do código
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # importante: necessário para fazer pull/rebase corretamente

      # 2️⃣ Configurar Python e Playwright
      - name: Instalar dependências
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install playwright
          playwright install chromium

      # 3️⃣ Criar o arquivo de login (cookies)
      - name: Criar google_login.json
        run: echo "$STORAGE_STATE_JSON" > google_login.json

      # 4️⃣ Rodar o scraper
      - name: Executar script assiny_scraper.py
        run: |
          source venv/bin/activate
          python assiny_scraper.py

      # 5️⃣ Commit e push automáticos
      - name: Commit e push de alterações
        run: |
          git config user.name "${GIT_USER_NAME}"
          git config user.email "${GIT_USER_EMAIL}"

          # Atualiza antes de commitar (evita rejeição de push)
          git pull --rebase origin main || true

          git add valor_assiny.csv state/latest.json || true
          git commit -m "update: valores atualizados pelo scraper" || echo "Nada a commitar"
          git push origin main || (
            echo "⚠️ Falha no push direto — tentando forçar atualização"
            git pull --rebase origin main
            git push origin main
          )
